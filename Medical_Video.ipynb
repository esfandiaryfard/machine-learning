{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esfandiaryfard/machine-learning/blob/main/Medical_Video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DGsRvsj6WlT",
        "outputId": "edf0b888-ae14-4067-ef68-ce625a243605"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikit-video\n",
            "  Downloading scikit_video-1.1.11-py2.py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from scikit-video) (1.10.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from scikit-video) (8.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from scikit-video) (1.22.4)\n",
            "Installing collected packages: scikit-video\n",
            "Successfully installed scikit-video-1.1.11\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting monai\n",
            "  Downloading monai-1.1.0-202212191849-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from monai) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.8/dist-packages (from monai) (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.8->monai) (4.5.0)\n",
            "Installing collected packages: monai\n",
            "Successfully installed monai-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-video\n",
        "!pip install monai\n",
        "from google.colab import drive\n",
        "drive = drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6byXQ-Z-Q0dD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.parameter import Parameter\n",
        "from skvideo import io\n",
        "from tqdm.gui import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrubaqhWdw2c"
      },
      "outputs": [],
      "source": [
        "batchsize = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5e_vcZC7EKh"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import json\n",
        "from torch.utils.data import DataLoader\n",
        "from monai.data import CacheDataset, DataLoader\n",
        "from monai.transforms import (\n",
        "    Compose,\n",
        "    MapTransform,\n",
        "    AddChannelD,\n",
        "    RepeatChannelD,\n",
        "    LoadImageD,\n",
        "    ScaleIntensityD,\n",
        "    NormalizeIntensityD,\n",
        "    RandFlipD,\n",
        "    RandRotate90D,\n",
        "    ResizeD,\n",
        "    ResizeWithPadOrCropD,\n",
        "    ToTensorD)\n",
        "from monai.config import KeysCollection\n",
        "import copy\n",
        "import argparse\n",
        "\n",
        "class AppendRootDirD(MapTransform):\n",
        "    def __init__(self, keys: KeysCollection, root_dir):\n",
        "        super().__init__(keys)\n",
        "        self.root_dir = root_dir\n",
        "    \n",
        "    def __call__(self, data):\n",
        "        d = copy.deepcopy(data)\n",
        "        for k in self.keys:\n",
        "            d[k] = os.path.join(self.root_dir,d[k])\n",
        "        return d\n",
        "\n",
        "class FFRDataset3D(CacheDataset):\n",
        "    def __init__(self, root_dir, split_path, transforms, cache_num = sys.maxsize, cache_rate=1.0, num_workers=1):    \n",
        "        if not os.path.isdir(root_dir):\n",
        "            raise ValueError(\"Root directory root_dir must be a directory.\")\n",
        "        \n",
        "        with open(split_path) as fp:\n",
        "           data=json.load(fp)\n",
        "\n",
        "        transforms_comp = Compose([\n",
        "            AppendRootDirD('image', root_dir),\n",
        "            transforms,\n",
        "            ])\n",
        "        super().__init__(data, transforms_comp, cache_num=cache_num, cache_rate=cache_rate, num_workers=num_workers)\n",
        "\n",
        "def get_loader(args):\n",
        "    transforms = Compose([\n",
        "        LoadImageD(['image']),\n",
        "        AddChannelD(['image']),\n",
        "        ResizeD(['image'], spatial_size = args.resize),\n",
        "        ScaleIntensityD(['image']),\n",
        "        NormalizeIntensityD(\n",
        "            ['image'], \n",
        "            subtrahend=args.mean, \n",
        "            divisor=args.std, \n",
        "            channel_wise=True\n",
        "            ),\n",
        "        ResizeWithPadOrCropD(['image'], spatial_size=args.pad, method=\"end\"),\n",
        "        ToTensorD(['image']),\n",
        "        RandFlipD(['image'], prob = 0.5, spatial_axis=1),\n",
        "        RandFlipD(['image'], prob = 0.5, spatial_axis=2),\n",
        "        RandRotate90D(['image'], prob = 0.5, spatial_axes=(1,2))\n",
        "        ])\n",
        "\n",
        "    dataset = FFRDataset3D(\n",
        "        root_dir = args.root_dir, \n",
        "        split_path = args.split_path, \n",
        "        transforms = transforms\n",
        "        )\n",
        "    \n",
        "    loader = DataLoader(dataset,\n",
        "                        batch_size = args.batch_size,\n",
        "                        num_workers=0,\n",
        "                        shuffle=args.shuffle)\n",
        "\n",
        "    return loader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "if __name__ == '__main__':\n",
        "    args={}\n",
        "    args['resize'] = (-1,256,256) # dataset max 256x256\n",
        "    args['pad'] = (50,256,256)\n",
        "    args['mean'] = (0.5173,)\n",
        "    args['std'] = (0.1908,)\n",
        "    args['root_dir'] = os.path.join('drive/MyDrive/dataset/')\n",
        "    args['split_path'] = os.path.join('drive/MyDrive/dataset/dataset.json')\n",
        "    args['batch_size'] = batchsize\n",
        "    args['shuffle'] = True\n",
        "    args = argparse.Namespace(**args)\n",
        "\n",
        "    # Dataset e Loader\n",
        "    loader = get_loader(args)\n",
        "\n",
        "    # Get samples\n",
        "    tmp = next(iter(loader))\n",
        "    plt.imshow(tmp['image'][1].numpy()[:,49,:,:].transpose(1,2,0).squeeze(), cmap=\"gray\") \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0oC_3OYjkNW"
      },
      "outputs": [],
      "source": [
        "tmp['image'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ERFgrj3j0CM"
      },
      "outputs": [],
      "source": [
        "tmp['image'].std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5F5J2ynvmoT1"
      },
      "outputs": [],
      "source": [
        "tmp['image'][1].numpy()[:,20,:,:].transpose(1,2,0).squeeze().shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_rMmkTe8Z0e"
      },
      "outputs": [],
      "source": [
        "class CLSTM_cell(nn.Module):\n",
        "    def __init__(self, n_filters):\n",
        "        \"\"\"Convolutional LSTM Cell\n",
        "\n",
        "        Args:\n",
        "            n_filters (int): Number of LSTM channels\n",
        "        \"\"\"\n",
        "        super(CLSTM_cell, self).__init__()\n",
        "        self.w_x = nn.Conv2d(n_filters, n_filters * 4, kernel_size=3,\n",
        "                             padding=1)\n",
        "        self.w_h = nn.Conv2d(n_filters, n_filters * 4, kernel_size=3,\n",
        "                             padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x, h=None, c=None):\n",
        "        xifoc = self.w_x(x)\n",
        "        xi, xf, xo, xc = xifoc.chunk(4, dim=1)\n",
        "        if h is not None:\n",
        "            hi, hf, ho, hc = self.w_h(h).chunk(4, dim=1)\n",
        "        else:\n",
        "            hi, hf, ho, hc = torch.zeros_like(xifoc).chunk(4, dim=1)\n",
        "\n",
        "        if c is None:\n",
        "            c = torch.zeros_like(x)\n",
        "\n",
        "        ci = torch.sigmoid(xi + hi)\n",
        "        cf = torch.sigmoid(xf + hf)\n",
        "        co = torch.sigmoid(xo + ho)\n",
        "        cc = cf * c + ci * torch.tanh(xc + hc)\n",
        "        ch = torch.tanh(cc) * co\n",
        "\n",
        "        return ch, cc\n",
        "\n",
        "\n",
        "class CLSTM(nn.Module):\n",
        "    def __init__(self, n_filters, n_frames):\n",
        "        \"\"\"Full Convolutional LSTM\n",
        "\n",
        "        Args:\n",
        "            n_filters (int): Number of LSTM channels\n",
        "            n_frames (int): Frames to generate\n",
        "        \"\"\"\n",
        "        super(CLSTM, self).__init__()\n",
        "        self.cell = CLSTM_cell(n_filters)\n",
        "        self.n_frames = n_frames\n",
        "\n",
        "    def forward(self, z):\n",
        "        # Assume z is in proper convolutional shape\n",
        "        out = torch.stack([torch.zeros_like(z)]*self.n_frames, dim=1)\n",
        "\n",
        "        h, c = None, None\n",
        "        for i in range(self.n_frames):\n",
        "            h, c = self.cell(z, h, c)\n",
        "            out[:, i] = h\n",
        "            z = torch.zeros_like(z)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, cin, cout):\n",
        "        \"\"\"Upscale and convolutions in ResNet setup\n",
        "\n",
        "        Args:\n",
        "            cin (int): in channels\n",
        "            cout (int): out channels\n",
        "        \"\"\"\n",
        "        super(Up, self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # define main branch\n",
        "        self.upsample = nn.Upsample(scale_factor=2)\n",
        "        self.bn1 = nn.BatchNorm2d(cin)\n",
        "        self.convm1 = nn.Conv2d(cin, cout, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(cout)\n",
        "        self.convm2 = nn.Conv2d(cout, cout, kernel_size=3, padding=1)\n",
        "\n",
        "        # define skip branch\n",
        "        self.sconv = nn.Conv2d(cin, cout, kernel_size=1)\n",
        "\n",
        "        # initialize\n",
        "        nn.init.xavier_uniform_(self.convm1.weight, gain=math.sqrt(2))\n",
        "        nn.init.xavier_uniform_(self.convm2.weight, gain=math.sqrt(2))\n",
        "        nn.init.xavier_uniform_(self.sconv.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # compute main\n",
        "        h = self.bn1(x)\n",
        "        h = self.relu(h)\n",
        "        h = self.upsample(h)\n",
        "        h = self.convm1(h)\n",
        "        h = self.bn2(h)\n",
        "        h = self.relu(h)\n",
        "        h = self.convm2(h)\n",
        "\n",
        "        # compute skip\n",
        "        s = self.upsample(x)\n",
        "        s = self.sconv(s)\n",
        "\n",
        "        return h + s\n",
        "\n",
        "\n",
        "class Render(nn.Module):\n",
        "    def __init__(self, cin, colors=3):\n",
        "        \"\"\"Render an image given the parameters\n",
        "\n",
        "        Args:\n",
        "            cin (int): in channels\n",
        "            colors (int, optional): Color channels. Defaults to 3.\n",
        "        \"\"\"\n",
        "        super(Render, self).__init__()\n",
        "        self.bn = nn.BatchNorm2d(cin)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv = nn.Conv2d(cin, colors, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv(x)\n",
        "        x = torch.tanh(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Generator_CLSTM(nn.Module):\n",
        "    def __init__(self, z_dim=256,\n",
        "                 tempc=1024,\n",
        "                 zt_dim=3,\n",
        "                 upchannels=[512, 256, 128],\n",
        "                 subchannels=[64, 32, 32],\n",
        "                 n_frames=16,\n",
        "                 colors=3):\n",
        "        \"\"\"Full generator CLSTM model\n",
        "\n",
        "        Args:\n",
        "            z_dim (int, optional): Latent z. Defaults to 256.\n",
        "            tempc (int, optional): CLSTM channels. Defaults to 1024.\n",
        "            zt_dim (int, optional): CLSTM window size. Defaults to 3.\n",
        "            upchannels (list, optional): Defaults to [512, 256, 128].\n",
        "            subchannels (list, optional): Defaults to [64, 32, 32].\n",
        "            n_frames (int, optional): Frames to generate. Defaults to 16.\n",
        "            colors (int, optional): Number of colors. Defaults to 3.\n",
        "        \"\"\"\n",
        "        super(Generator_CLSTM, self).__init__()\n",
        "        assert len(subchannels) == 3\n",
        "        self.tempc = tempc\n",
        "        self.zt_dim = zt_dim\n",
        "        self.colors = colors\n",
        "\n",
        "        self.fc = nn.Linear(z_dim, zt_dim**2 * tempc)\n",
        "        self.temp = CLSTM(tempc, n_frames)\n",
        "\n",
        "        self.build = nn.Sequential()\n",
        "        for i in range(len(upchannels)):\n",
        "            if not i:\n",
        "                self.build.add_module('Up1', Up(tempc, upchannels[0]))\n",
        "            else:\n",
        "                self.build.add_module(f'Up{i+1}', Up(upchannels[i-1],\n",
        "                                      upchannels[i]))\n",
        "\n",
        "        self.buildr = Render(upchannels[-1], colors=colors)\n",
        "\n",
        "        self.sup1 = Up(upchannels[-1], subchannels[0])\n",
        "        self.sup1r = Render(subchannels[0], colors=colors)\n",
        "        self.sup2 = Up(subchannels[0], subchannels[1])\n",
        "        self.sup2r = Render(subchannels[1], colors=colors)\n",
        "        self.sup3 = Up(subchannels[1], subchannels[2])\n",
        "        self.sup3r = Render(subchannels[2], colors=colors)\n",
        "\n",
        "    def subsample(self, h, N, T, frames=4):\n",
        "        # to vid\n",
        "        _, C, H, W = h.shape\n",
        "        h = h.view(N, T, C, H, W)\n",
        "        h = h[:, np.random.randint(min(frames, T))::frames]\n",
        "        N, T, C, H, W = h.shape\n",
        "        # to train\n",
        "        h = h.contiguous().view(N * T, C, H, W)\n",
        "        return h, T\n",
        "\n",
        "    def forward(self, z, test=False):\n",
        "        \"\"\"Compute generator forward pass\n",
        "\n",
        "        Args:\n",
        "            z (torch.Tensor): Latent z [batch_size, z_dim]\n",
        "            test (bool, optional): Produce test videos. Defaults to False.\n",
        "\n",
        "        Returns:\n",
        "            list(torch.Tensor) or torch.Tensor: Subsampled or regular videos\n",
        "        \"\"\"\n",
        "        h = self.fc(z)\n",
        "        h = h.view(-1, self.tempc, self.zt_dim, self.zt_dim)\n",
        "        h = self.temp(h)\n",
        "        N, T, C, H, W = h.shape\n",
        "        h = h.view(N*T, C, H, W)\n",
        "        h = self.build(h)\n",
        "\n",
        "        outsize = self.zt_dim * 2 ** (len(self.build) + 3)\n",
        "\n",
        "        if test:\n",
        "            h = self.sup1(h)\n",
        "            h = self.sup2(h)\n",
        "            h = self.sup3(h)\n",
        "            h = self.sup3r(h).view(N, T, self.colors, outsize,\n",
        "                                   outsize).transpose(1, 2)\n",
        "\n",
        "            return h\n",
        "        else:\n",
        "            # render 1st\n",
        "            x1 = self.buildr(h).view(N, T, self.colors, outsize // 8,\n",
        "                                     outsize // 8)\n",
        "            h, T = self.subsample(h, N, T)\n",
        "            h = self.sup1(h)\n",
        "            # render 2nd\n",
        "            x2 = self.sup1r(h).view(N, T, self.colors, outsize // 4,\n",
        "                                    outsize // 4)\n",
        "            h, T = self.subsample(h, N, T)\n",
        "            h = self.sup2(h)\n",
        "            # render 3rd\n",
        "            x3 = self.sup2r(h).view(N, T, self.colors, outsize // 2,\n",
        "                                    outsize // 2)\n",
        "            h, T = self.subsample(h, N, T)\n",
        "            h = self.sup3(h)\n",
        "            # render 4th\n",
        "            x4 = self.sup3r(h).view(N, T, self.colors, outsize, outsize)\n",
        "\n",
        "        return x1, x2, x3, x4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yz-aTiQiVzZy"
      },
      "outputs": [],
      "source": [
        "class MiniBatchDiscrimination(nn.Module):\n",
        "    def __init__(self, A, B, C, batch_size):\n",
        "        super(MiniBatchDiscrimination, self).__init__()\n",
        "        self.feat_num = A\n",
        "        self.out_size = B\n",
        "        self.row_size = C\n",
        "        self.N = batch_size\n",
        "        self.T = nn.Parameter(torch.Tensor(A, B, C))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Output matrices after matrix multiplication\n",
        "        M = x.matmul(self.T.view(self.feat_num, self.out_size * self.row_size)).view(-1, self.out_size, self.row_size)\n",
        "        out = torch.zeros(self.N, self.out_size, device=x.device)\n",
        "        for k in range(self.N):\n",
        "            c = torch.exp(-torch.norm(M[k] - M, p=2, dim=1)) # exp(-L2 Norm of Rows difference)\n",
        "            if k != 0 and k != self.N - 1:\n",
        "                out[k] = torch.sum(c[:k], dim=0) + torch.sum(c[k:-1], dim=0)\n",
        "            else:\n",
        "                if k == 0:\n",
        "                    out[k] = torch.sum(c[1:], dim=0)\n",
        "                else:\n",
        "                    out[k] = torch.sum(c[:self.N - 1], dim=0)\n",
        "        return out\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stddev = 1 / self.feat_num\n",
        "        self.T.data.uniform_(-stddev, stddev)\n",
        "\n",
        "class OptimizedDiscBlock(nn.Module):\n",
        "    def __init__(self, cin, cout):\n",
        "        \"\"\"Optimized Discriminator Block\n",
        "\n",
        "        Args:\n",
        "            cin (int): in channels\n",
        "            cout (int): out channels\n",
        "        \"\"\"\n",
        "        super(OptimizedDiscBlock, self).__init__()\n",
        "        self.c1 = nn.Conv3d(cin, cout, kernel_size=3, padding=1)\n",
        "        self.c2 = nn.Conv3d(cout, cout, kernel_size=3, padding=1)\n",
        "        self.c_sc = nn.Conv3d(cin, cout, kernel_size=1, padding=0)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.avgp2d = nn.AvgPool3d(kernel_size=(1, 2, 2))\n",
        "\n",
        "        # init\n",
        "        nn.init.xavier_uniform_(self.c1.weight, gain=math.sqrt(2))\n",
        "        nn.init.xavier_uniform_(self.c2.weight, gain=math.sqrt(2))\n",
        "        nn.init.xavier_uniform_(self.c_sc.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.c1(x)\n",
        "        h = self.relu(h)\n",
        "        h = self.c2(h)\n",
        "        h = self.avgp2d(h)\n",
        "\n",
        "        s = self.avgp2d(x)\n",
        "        s = self.c_sc(s)\n",
        "\n",
        "        return h + s\n",
        "\n",
        "\n",
        "class DisBlock(nn.Module):\n",
        "    def __init__(self, cin, cout):\n",
        "        \"\"\"Discriminator Block\n",
        "\n",
        "        Args:\n",
        "            cin (int): in channels\n",
        "            cout (int): out channels\n",
        "        \"\"\"\n",
        "        super(DisBlock, self).__init__()\n",
        "        self.c1 = nn.Conv3d(cin, cin, kernel_size=3, padding=1)\n",
        "        self.c2 = nn.Conv3d(cin, cout, kernel_size=3, padding=1)\n",
        "        self.s_sc = nn.Conv3d(cin, cout, kernel_size=1, padding=0)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # init\n",
        "        nn.init.xavier_uniform_(self.c1.weight, gain=math.sqrt(2))\n",
        "        nn.init.xavier_uniform_(self.c2.weight, gain=math.sqrt(2))\n",
        "        nn.init.xavier_uniform_(self.s_sc.weight)\n",
        "\n",
        "    def downsample(self, x):\n",
        "        ksize = [(2 if 1 < k else 1) for k in x.shape[2:]]\n",
        "        pad = [(0 if k % 2 == 0 else 1) for k in x.shape[2:]][::-1]\n",
        "        padf = []\n",
        "        for p in pad:\n",
        "            padf.append(p)\n",
        "            padf.append(p)\n",
        "        x = F.pad(x, padf)\n",
        "        return F.avg_pool3d(x, kernel_size=ksize, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.relu(x)\n",
        "        h = self.c1(h)\n",
        "        h = self.relu(h)\n",
        "        h = self.c2(h)\n",
        "        h = self.downsample(h)\n",
        "\n",
        "        s = self.s_sc(x)\n",
        "        s = self.downsample(s)\n",
        "        return h + s\n",
        "\n",
        "\n",
        "class DisResNet(nn.Module):\n",
        "    def __init__(self, channels=[64, 128, 256, 512, 1024], colors=3):\n",
        "        \"\"\"Singular discriminator with multiple DisBlocks\n",
        "\n",
        "        Args:\n",
        "            channels (list, optional): Defaults to [64, 128, 256, 512, 1024].\n",
        "            colors (int, optional): Color channels. Defaults to 3.\n",
        "        \"\"\"\n",
        "        super(DisResNet, self).__init__()\n",
        "        self.convs = nn.Sequential()\n",
        "        self.colors = colors\n",
        "\n",
        "        for i in range(len(channels)):\n",
        "            if not i:\n",
        "                self.convs.add_module(\n",
        "                    'OptDisc',\n",
        "                    OptimizedDiscBlock(colors, channels[0])\n",
        "                )\n",
        "            else:\n",
        "                self.convs.add_module(\n",
        "                    f'Down{i}',\n",
        "                    DisBlock(channels[i-1], channels[i])\n",
        "                )\n",
        "        \n",
        "        self.mbd = MiniBatchDiscrimination(channels[-1], 64, 50, batchsize)  # added MBD module here\n",
        "        self.fc = nn.Linear(channels[-1] + 64, 1)  # modified input size of fc layer\n",
        "        nn.init.xavier_uniform_(self.fc.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.shape[2] == self.colors:\n",
        "            x = x.transpose(1, 2)\n",
        "        h = self.convs(x)\n",
        "        h = torch.sum(h, dim=(2, 3, 4))\n",
        "        mbd_out = self.mbd(h.view(-1, self.mbd.feat_num)).cuda() # added mbd\n",
        "        h = torch.cat((h, mbd_out), dim=1)  # concatenate output of MBD module\n",
        "        h = self.fc(h)\n",
        "        return h\n",
        "\n",
        "class DisMultiResNet(nn.Module):\n",
        "    def __init__(self, layers=4, channels=[64, 128, 256, 512, 1024], colors=3):\n",
        "        \"\"\"Multiple Discriminators to run inference on\n",
        "\n",
        "        Args:\n",
        "            layers (int, optional): Discriminator Count. Defaults to 4.\n",
        "            channels (list, optional): Defaults to [64, 128, 256, 512, 1024].\n",
        "            colors (int, optional): Color channels. Defaults to 3.\n",
        "        \"\"\"\n",
        "        super(DisMultiResNet, self).__init__()\n",
        "        self.layers = layers\n",
        "        self.res = nn.ModuleList(\n",
        "            [DisResNet(channels, colors) for _ in range(layers)]\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        assert self.layers == len(x)\n",
        "        out = [self.res[i](x[i]) for i in range(self.layers)]\n",
        "        out = torch.cat(out, dim=0)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2HgbhCOz9PO"
      },
      "outputs": [],
      "source": [
        "def genSamples(g, n=8, e=1):\n",
        "    with torch.no_grad():\n",
        "        s = g(torch.rand((n**2, 256), device='cuda')*2-1,\n",
        "              test=True).cpu().detach().numpy()\n",
        "    out = np.zeros((1, 20, 64*n, 64*n))\n",
        "\n",
        "    for j in range(n):\n",
        "        for k in range(n):\n",
        "            out[:, :, 64*j:64*(j+1), 64*k:64*(k+1)] = s[j*n + k, 0, :, :, :]\n",
        "\n",
        "    out = out.transpose((1, 2, 3, 0))\n",
        "    out = (np.concatenate([out, out, out], axis=3)+1) / 2 * 255\n",
        "    io.vwrite(f'drive/MyDrive/medical_results/gensamples_id{e}.gif', out)\n",
        "\n",
        "\n",
        "def subsample_real(h, frames=4):\n",
        "    h = h[:, np.random.randint(min(frames, h.shape[1]))::frames]\n",
        "    return h\n",
        "\n",
        "\n",
        "def full_subsample_real(h, frames=4):\n",
        "    out = []\n",
        "    for i in range(4):\n",
        "        if i:\n",
        "            out.append(subsample_real(out[i-1], frames=frames))\n",
        "        else:\n",
        "            out.append(h)\n",
        "    for i in range(4):\n",
        "        for j in range(3-i):\n",
        "            out[i] = F.avg_pool3d(out[i], kernel_size=(1, 2, 2))\n",
        "    return out\n",
        "\n",
        "\n",
        "def zero_centered_gp(real_data, pr):\n",
        "    gradients = torch.autograd.grad(outputs=pr, inputs=real_data,\n",
        "                                    grad_outputs=torch.ones_like(pr),\n",
        "                                    create_graph=True, retain_graph=True)\n",
        "\n",
        "    return sum([torch.sum(torch.square(g)) for g in gradients])\n",
        "\n",
        "\n",
        "def train():\n",
        "    epochs = 3000\n",
        "    batch_size = batchsize\n",
        "    lambda_val = 0.5\n",
        "\n",
        "    # gen model\n",
        "    dis = DisMultiResNet(channels=[32,64, 128, 256], colors=1).cuda()\n",
        "    gen = Generator_CLSTM(\n",
        "        tempc=256,\n",
        "        zt_dim=4,\n",
        "        upchannels=[128],\n",
        "        subchannels=[64, 32, 32],\n",
        "        n_frames=20,\n",
        "        colors=1\n",
        "    ).cuda()\n",
        "\n",
        "    disOpt = torch.optim.Adam(dis.parameters(), lr=5e-5, betas=(0, 0.9))\n",
        "    genOpt = torch.optim.Adam(gen.parameters(), lr=1e-4, betas=(0, 0.9))\n",
        "\n",
        "    # train\n",
        "    # note on loss function: within the current github repo they\n",
        "    # employ softplus linear loss, if the normal cross entropy\n",
        "    # is desired one may simply change the comments\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        # discriminator\n",
        "        \n",
        "        disOpt.zero_grad()\n",
        "        real = next(iter(loader))\n",
        "        real = real['image'].transpose(1,2)\n",
        "        real = real.cuda()\n",
        "        real = real.to(dtype=torch.float32) / 255 * 2 - 1\n",
        "        real = full_subsample_real(real)\n",
        "        for i in real:\n",
        "            i.requires_grad = True\n",
        "        \n",
        "        if epoch > 100:\n",
        "          pr = dis(real)\n",
        "          dis_loss = zero_centered_gp(real, pr) * lambda_val\n",
        "          with torch.no_grad():\n",
        "              fake = gen(torch.rand((batch_size, 256), device='cuda')*2-1)\n",
        "          pf = dis(fake)\n",
        "          dis_loss += torch.mean(F.softplus(-pr)) + torch.mean(F.softplus(pf))\n",
        "          dis_loss.backward() \n",
        "          disOpt.step()\n",
        "\n",
        "        # generator\n",
        "        genOpt.zero_grad()\n",
        "        fake = gen(torch.rand((batch_size, 256), device='cuda')*2-1)\n",
        "        pf = dis(fake)\n",
        "        gen_loss = torch.mean(F.softplus(-pf))\n",
        "        gen_loss.backward()\n",
        "        genOpt.step()\n",
        "\n",
        "        dis = dis_loss.item() if dis_los is not None else 0\n",
        "        # log results\n",
        "        print('Epoch', epoch, 'Dis', dis_loss.item(), 'Gen', gen_loss.item())\n",
        "        if epoch % 100 == 0:\n",
        "            genSamples(gen, e=epoch)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HY5EDZP1oBOq"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/Zasder3/Tganv2-PyTorch-Train-Sparsely--Generate-Densely.git"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNI1Gt+LjUePTpWve8HDG+N",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}