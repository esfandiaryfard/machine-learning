{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esfandiaryfard/machine-learning/blob/main/freinds.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tn-AZuyLl2ry",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d81e40c8-781d-47ec-d57f-581bec94b8c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('script.txt', <http.client.HTTPMessage at 0x7f0c121f3390>)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn; cudnn.benchmark = True\n",
        "import urllib\n",
        "\n",
        "urllib.request.urlretrieve(\n",
        "    'https://drive.google.com/uc?export=download&id=1f10Zoa_Lqg82-BnFLCHavVWig3Ei2fMV', \n",
        "    'script.txt'\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbC-kgGLIFoT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "86e7fe1d-2543-4976-ed7d-04b0f8efef12"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Monica: There's nothing to tell |exclamation|  He's just some guy I work with |exclamation|  |newline| Joey: C'mon |comma|  you're going out with the guy |exclamation|  There's gotta be something wron\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "data_path = \"script.txt\"\n",
        "batch_size = 8\n",
        "batch_seq_len = 64\n",
        "embed_size = 512\n",
        "rnn_size = 1024\n",
        "\n",
        "with open(data_path) as f:\n",
        "    text = f.read()\n",
        "text = text[180:]\n",
        "\n",
        "token_dict = {\".\": \"|fullstop|\",\n",
        "              \",\": \"|comma|\",\n",
        "              \"\\\"\": \"|quote|\",\n",
        "              \";\": \"|semicolon|\",\n",
        "              \"!\": \"|exclamation|\",\n",
        "              \"?\": \"|question|\",\n",
        "              \"(\": \"|leftparen|\",\n",
        "              \")\": \"|rightparen|\",\n",
        "              \"--\": \"|dash|\",\n",
        "              \"\\n\": \"|newline|\"\n",
        "}\n",
        "for punct, token in token_dict.items():\n",
        "    text = text.replace(punct, f' {token} ')\n",
        "\n",
        "text[:200]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDuSgspSIihn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfb04be5-d943-405c-c833-3523fc486d50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 umIm\n",
            "1 stunned/incredulous\n",
            "2 bangs\n",
            "3 bag\n",
            "4 Y'\n"
          ]
        }
      ],
      "source": [
        "words = text.split(\" \")\n",
        "words = [word for word in words if len(word) > 0]\n",
        "vocab = list(set(words))\n",
        "\n",
        "for i, w in enumerate(vocab[:5]):\n",
        "  print(i, w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1OMDvTQInfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "221ef9d5-c2d9-4b46-f1df-ce66e5e65418"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27750\n"
          ]
        }
      ],
      "source": [
        "vocab_to_int = {word: i for i,word in enumerate(vocab)}\n",
        "int_to_vocab = {i: word for i,word in enumerate(vocab)}\n",
        "\n",
        "num_words = len(vocab)\n",
        "print(num_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fg2xLOeroC6",
        "outputId": "21a6dcc7-000a-46e7-9303-0d8a6ecadd12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1207503\n"
          ]
        }
      ],
      "source": [
        "print(len([word for word in text.split(\" \") if len(word) > 0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3edY1kymIsNO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "621312f2-9105-4ce0-e497-b919f148fb9a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1207503"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "text_ints = [vocab_to_int[word] for word in text.split(\" \") if len(word) > 0]\n",
        "len(text_ints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtc-NZ4aI3-o",
        "outputId": "d49ea335-825f-49b1-d056-679203442a2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "389.8944139489829\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "scene = re.findall(r'\\[Scene.*?\\]', text)\n",
        "num_scenes = len(scene)\n",
        "print(len(text_ints)/num_scenes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4T9jb4MDtsgU",
        "outputId": "8c42ae53-aaee-4b3c-d079-ff52974353b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Monica:', \"There's\", 'nothing', 'to', 'tell']\n",
            "[\"There's\", 'nothing', 'to', 'tell']\n"
          ]
        }
      ],
      "source": [
        "new_text = [word for word in text.split(\" \") if len(word) > 0]\n",
        "inputs = new_text[:5]\n",
        "target = new_text[1:5]\n",
        "\n",
        "print(inputs)\n",
        "print(target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KNKo53rI7B4"
      },
      "outputs": [],
      "source": [
        "scene_length = 400"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tAWBTdBI9EI"
      },
      "outputs": [],
      "source": [
        "def get_batches(text_ints, scene_length, batch_size, batch_seq_len):\n",
        "    num_scenes = len(text_ints)//scene_length\n",
        "    text_targets = text_ints[1:] + [text_ints[0]]\n",
        "    scene_inputs = [\n",
        "                    text_ints[i * scene_length : (i+1) * scene_length] for i in \n",
        "                    range(num_scenes)\n",
        "                    ]\n",
        "    scene_targets = [\n",
        "                     text_targets[i*scene_length:(i+1)*scene_length] for i in \n",
        "                     range(num_scenes)\n",
        "                     ]\n",
        "    num_mini_sequences = scene_length//batch_seq_len\n",
        "    scene_inputs = [\n",
        "                    [scene[i*batch_seq_len:(i+1)*batch_seq_len] for i in \n",
        "                     range(num_mini_sequences)] for scene in scene_inputs\n",
        "                    ]\n",
        "    scene_targets = [\n",
        "                     [scene[i*batch_seq_len:(i+1)*batch_seq_len] for i in \n",
        "                      range(num_mini_sequences)] for scene in scene_targets\n",
        "                     ]\n",
        "    num_batch_groups = len(scene_inputs)//batch_size\n",
        "    batches = []\n",
        "    for i in range(num_batch_groups):\n",
        "        group_scene_inputs = scene_inputs[i*batch_size:(i+1)*batch_size]\n",
        "        group_scene_targets = scene_targets[i*batch_size:(i+1)*batch_size]\n",
        "        for j in range(num_mini_sequences):\n",
        "            reset_state = (j == 0)\n",
        "            batch_inputs = torch.LongTensor(\n",
        "                [group_scene_inputs[k][j] for k in range(batch_size)]\n",
        "                )\n",
        "            batch_targets = torch.LongTensor(\n",
        "                [group_scene_targets[k][j] for k in range(batch_size)]\n",
        "                )\n",
        "            batches.append((reset_state, batch_inputs, batch_targets))\n",
        "    return batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LR-0uWcmJGsD",
        "outputId": "4f726fae-1545-4d43-e9c0-b02de2d9dc6e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "batches = get_batches(text_ints, scene_length, batch_size, batch_seq_len)\n",
        "batches[0][1].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ydUKp0zrbhU",
        "outputId": "0873d05d-1616-4b0c-f9ff-8ab788acccc0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['me',\n",
              " '|exclamation|',\n",
              " '|newline|',\n",
              " '|leftparen|',\n",
              " 'The',\n",
              " 'scene',\n",
              " 'on',\n",
              " 'TV',\n",
              " 'has',\n",
              " 'changed',\n",
              " 'to',\n",
              " 'show',\n",
              " 'two',\n",
              " 'women',\n",
              " '|comma|',\n",
              " 'one']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "script = [  int_to_vocab[y.item()] for y in [x for x in batches[1][1][3]] ]\n",
        "script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bH4lAyqoJIAU"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_words, embed_size, rnn_size):\n",
        "        super().__init__()\n",
        "        self.rnn_size = rnn_size\n",
        "        self.state = None\n",
        "        self.embedding = nn.Embedding(num_words, embed_size)\n",
        "        self.rnn = nn.LSTM(embed_size, rnn_size, batch_first=True)\n",
        "        self.decoder = nn.Linear(rnn_size, num_words)\n",
        "        self.reset_next_state = False\n",
        "        \n",
        "    def reset_state(self):\n",
        "        self.reset_next_state = True\n",
        "        \n",
        "    def forward(self, x):\n",
        "        if self.reset_next_state:\n",
        "            self.state = (\n",
        "                x.new_zeros(1, x.size(0), self.rnn_size).float(), \n",
        "                x.new_zeros(1, x.size(0), self.rnn_size).float())\n",
        "            self.reset_next_state = False\n",
        "        x = self.embedding(x)\n",
        "        state = self.state if self.state is not None else None\n",
        "        x, state = self.rnn(x, state)\n",
        "        self.state = (state[0].data, state[1].data)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Qft7-Y6JSbp"
      },
      "outputs": [],
      "source": [
        "model = Model(num_words, embed_size, rnn_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5CdW5cJJS5x"
      },
      "outputs": [],
      "source": [
        "# Setup device\n",
        "dev = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIWlZHS3JVHZ",
        "outputId": "77fee079-26f4-4b2e-96f1-29b5cb307263",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-34b204928147>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Move model to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    905\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m     def register_backward_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    903\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    904\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 905\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
          ]
        }
      ],
      "source": [
        "# Move model to device\n",
        "model = model.to(dev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obYaHGafJXBy"
      },
      "outputs": [],
      "source": [
        "# Define script generation function\n",
        "def generate_script(model, seq_len, script_start):\n",
        "    # Convert punctuaction in script start\n",
        "    for punct, token in token_dict.items():\n",
        "        script_start = script_start.replace(punct, f' {token} ')\n",
        "    # Convert script start text to ints\n",
        "    script_start = [vocab_to_int[word] for word in script_start.split(\" \") if len(word) > 0]\n",
        "    # Initialize output words/tokens\n",
        "    script = script_start[:]\n",
        "    # Convert script start to tensor (BxS = 1xS)\n",
        "    script_start = torch.LongTensor(script_start).unsqueeze_(0)\n",
        "    # Process script start and generate the rest of the script\n",
        "    model.eval()\n",
        "    model.reset_state()\n",
        "    input = script_start\n",
        "    for i in range(seq_len - script_start.size(1) + 1): # we include script_start as one of the generation steps\n",
        "        # Copy input to device\n",
        "        input = input.to(dev)\n",
        "        # Pass to model\n",
        "        output = model(input) # 1xSxV\n",
        "        # Convert to word indexes\n",
        "        words = output.max(2)[1] # 1xS\n",
        "        words = words[0] # S\n",
        "        # Add each word to script\n",
        "        for j in range(words.size(0)):\n",
        "            script.append(words[j].item())\n",
        "        # Prepare next input\n",
        "        input = torch.LongTensor([words[-1]]).unsqueeze(0) # 1xS = 1x1\n",
        "    # Convert word indexes to text\n",
        "    script = ' '.join([int_to_vocab[x] for x in script])\n",
        "    # Convert punctuation tokens to symbols\n",
        "    for punct,token in token_dict.items():\n",
        "        script = script.replace(f\"{token}\", punct)\n",
        "    # Return\n",
        "    return script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyxocXwfJZxp"
      },
      "outputs": [],
      "source": [
        "generate_script(model, 20, \"Monica: You can't believe it\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9MBHBYSJdjI"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jz_DsW0FJeyQ"
      },
      "outputs": [],
      "source": [
        "# Initialize training history\n",
        "loss_history = []\n",
        "# Start training\n",
        "for epoch in range(25):\n",
        "    # Initialize accumulators for computing average loss/accuracy\n",
        "    epoch_loss_sum = 0\n",
        "    epoch_loss_cnt = 0\n",
        "    # Set network mode\n",
        "    model.train()\n",
        "    # Process all batches\n",
        "    for i,batch in enumerate(batches):\n",
        "        # Parse batch\n",
        "        reset_state, input, target = batch\n",
        "        # Check reset state\n",
        "        if reset_state:\n",
        "            model.reset_state()\n",
        "        # Move to device\n",
        "        input = input.to(dev)\n",
        "        target = target.to(dev)\n",
        "        # Forward\n",
        "        output = model(input)\n",
        "        # Compute loss\n",
        "        output = output.view(-1, num_words)\n",
        "        target = target.view(-1)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        # Update loss sum\n",
        "        epoch_loss_sum += loss.item()\n",
        "        epoch_loss_cnt += 1\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    # Shift sequence and recompute batches\n",
        "    shift_point = random.randint(1, len(text_ints)-1)\n",
        "    text_ints = text_ints[:shift_point] + text_ints[shift_point:]\n",
        "    batches = get_batches(text_ints, scene_length, batch_size, batch_seq_len)\n",
        "    # Epoch end - compute average epoch loss\n",
        "    avg_loss = epoch_loss_sum/epoch_loss_cnt\n",
        "    print(f\"Epoch: {epoch+1}, loss: {epoch_loss_sum/epoch_loss_cnt:.4f}\")\n",
        "    print(\"Test sample:\")\n",
        "    print(\"---------------------------------------------------------------\")\n",
        "    print(generate_script(model, scene_length, \"Ross:\"))\n",
        "    print(\"---------------------------------------------------------------\")\n",
        "    # Add to histories\n",
        "    loss_history.append(avg_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbYpKsv8Jk0v"
      },
      "outputs": [],
      "source": [
        "# Generate script\n",
        "print(generate_script(model, scene_length, \"Monica: You can't believe it\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ????? How we can omprove this shit? batch? epochs? data? I am not quite convinced\n",
        "# can we devide based on real devider?"
      ],
      "metadata": {
        "id": "3ba_oErBU2SE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "freinds.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}